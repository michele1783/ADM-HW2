{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Homework_2.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "dhlsWRYSFF_D"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from functools import reduce"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "bYUpOM2nFF_G"
      },
      "source": [
        "## Functions\n",
        "Here we have defined all the useful functions to accomplish the request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "mGqM8VlVFF_I"
      },
      "source": [
        "def orario(lista, time_col):\n",
        "    list_time = [datetime.strptime(t, '%H:%M:%S').time() for t in lista]\n",
        "    number_review = []\n",
        "    for i in range(0, len(list_time), 2):\n",
        "        number_review.append(len((time_col[(time_col >= list_time[i]) & (time_col <= list_time[i + 1])])))\n",
        "    xx = []\n",
        "    for i in range(0, len(list_time), 2):\n",
        "        xx.append(str(list_time[i].hour))\n",
        "    #xx = ['6am', '11am', '2pm', '5pm', '8pm', '12am', '3am']\n",
        "    plt.bar(xx, number_review, color = 'salmon')\n",
        "    plt.yscale('log')\n",
        "    plt.yticks([2000000, 2500000, 3000000, 3500000, 4000000])\n",
        "    plt.title('Number of review for each interval of time')\n",
        "    plt.xlabel('Intervals')\n",
        "    plt.ylabel('Number of review')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "xWuiE0B9FF_J"
      },
      "source": [
        "def parsedate(time_as_a_unix_timestamp):\n",
        "    return pd.to_datetime(time_as_a_unix_timestamp, unit = 's')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "pATWHUBHFF_J"
      },
      "source": [
        "def filtro(data, lingue):\n",
        "    a = pd.DataFrame(columns = dataset.columns)\n",
        "    for i in range(len(lingue)):\n",
        "        a = pd.concat([a, data[data.language == lingue[i]]])\n",
        "    return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "We93wxtgFF_K"
      },
      "source": [
        "# Load the dataset\n",
        "we load our dataset and using the function **parsedate** we have changed the format of our timestamp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "DpCdAy6yFF_K"
      },
      "source": [
        "dataset = pd.read_csv('steam_reviews.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "5Mvdx8YXFF_L"
      },
      "source": [
        "dataset = pd.read_csv('steam_reviews.csv', header='infer',\n",
        "parse_dates=['timestamp_created',\n",
        "'timestamp_updated', 'author.last_played'],\n",
        "date_parser = parsedate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9nuGAfIIFF_M"
      },
      "source": [
        "dataset.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "EqdUeCf0FF_M"
      },
      "source": [
        "dataset.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "RkqkHi2JFF_N"
      },
      "source": [
        "# RQ1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "zjXPMbjqFF_N"
      },
      "source": [
        "###  Exploratory Data Analysis (EDA)\n",
        "\n",
        "To try to better understand our dataset we have made a bunch of plots and tables in which we have tried to catch some information about these reviews received for the applications in Steam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "acNWGxvEFF_O"
      },
      "source": [
        "#### Application more reviewed: \n",
        "To start our analysis we have made a pie chart about applications more reviewed. In particular we have decided to pick the first thirty games more reviewed and understand how the number of rewiews is splitted between them. Indeed the percentage written in the slices of the pie plot is referred not to the total number of reviews but the to the sum of reviews written for these thirty more popular games. The choice of thirty is due to make cleaner the plot and because we are interested only in the more popular games. The most talked-about."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "YNTptLsuFF_O"
      },
      "source": [
        "a = pd.Series(dataset.groupby(\"app_name\").app_id.count().sort_values(ascending=False).head(30))\n",
        "plt.rcParams['figure.figsize'] = (10, 10)\n",
        "plt.pie(a,\n",
        "labels = a.index,\n",
        "explode = [0.1 for value in range(0, a.index.nunique())],\n",
        "shadow = True, autopct = '%.1f%%')\n",
        "plt.title('Application name', fontsize = 20)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "DVQyJ11aFF_O"
      },
      "source": [
        "#### Correlation matrix:\n",
        "Then we have tried to make a correlation matrix to understand if there are some variables correlated between them "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "cxJbHdKRFF_O"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(13,13)) \n",
        "sns.heatmap(dataset.corr(), cbar=True, annot = True, cmap='BrBG', linewidths=.3,fmt='.1g')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "JGfIFe98FF_P"
      },
      "source": [
        "We have noticed that there was not any particular correlation between columns except for the ones related to time played by the player therefore we have decided to see in depth these correlations to have clearer information about them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "gBIoh0_LFF_P"
      },
      "source": [
        "df = pd.DataFrame(dataset,columns=['author.playtime_forever','author.playtime_last_two_weeks',\\\n",
        "                                   'author.playtime_at_review'])\n",
        "corrMatrix = df.corr()\n",
        "sns.heatmap(corrMatrix, annot=True)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "wg5h7JJbFF_P"
      },
      "source": [
        "#### Time and Language:\n",
        "At this point we want to extract some information about the language of the reviews and time when they were written. We have divided the day in three parts: morning (8am-2pm), afternoon (2pm-10pm) and night (10pm-8am). \n",
        "So for each part of the day we have grouped the reviews by language, counted them and picked the ten languages more popular.\n",
        "\n",
        "In this way in our final barplot for each popular language we have the number of reviews written in each part of the day. We have also made a table to explain better the number obtained. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Ql-_arCbFF_P"
      },
      "source": [
        "arr_1 = np.array((dataset['timestamp_created'].dt.time.astype('str') >= \"08:00:00\")& (dataset['timestamp_created'].dt.time.astype('str') <= \"13:59:59\"))\n",
        "arr_2 = np.array((dataset['timestamp_created'].dt.time.astype('str') >= \"14:00:00\")& (dataset['timestamp_created'].dt.time.astype('str') <= \"21:59:59\"))\n",
        "arr_3 = np.array((dataset['timestamp_created'].dt.time.astype('str') >= \"22:00:00\")& (dataset['timestamp_created'].dt.time.astype('str') <= \"7:59:59\"))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7GccMGs4FF_Q"
      },
      "source": [
        "mattina = pd.Series(dataset[arr_1].groupby(\"language\").language.count().sort_values(ascending=False).head(10))\n",
        "pomeriggio = pd.Series(dataset[arr_2].groupby(\"language\").language.count().sort_values(ascending=False).head(10))\n",
        "notte = pd.Series(dataset[arr_3].groupby(\"language\").language.count().sort_values(ascending=False).head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "80CeQ8NKFF_Q"
      },
      "source": [
        "df = mattina.to_frame(name = \"8am-2pm\")\n",
        "df[\"2pm-10pm\"]=pomeriggio\n",
        "df[\"10pm-8am\"]=notte\n",
        "df['language'] = df.index\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "AFflrFqsFF_Q"
      },
      "source": [
        "df.set_index([\"language\"], drop = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "mfFmCMrkFF_Q"
      },
      "source": [
        "ax = df.plot(x = \"language\", kind ='bar', stacked = False, figsize =(10,8), alpha=1, rot=0)\n",
        "ax.set_yscale('log')\n",
        "ax.set_xlabel('languages')\n",
        "ax.set_ylabel(\"number reviews\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "iwV-oQFEFF_Q"
      },
      "source": [
        "In this stacked barplot we can see that the majority of the reviews are written during the afternoon while during the night fewer people usually write on Steam. The language more used as expected is English"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "uQfbxU7SFF_R"
      },
      "source": [
        "#### Viral Comments:\n",
        "In this table we have wanted to look at the ten reviews which have received more comments because we have thought that it could be interesting look at them to understand which comments are popular on Steam. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "EWPCV8K7FF_R"
      },
      "source": [
        "dataset_7 = dataset.sort_values(by=['comment_count'], ascending = False)\n",
        "dataset_7 = dataset_7.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "z-7cQr0UFF_R"
      },
      "source": [
        "dataset_7[[\"author.steamid\", \"language\", \"review\", \"comment_count\"]].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "kep68YBdFF_R"
      },
      "source": [
        "Unfortunately the majority of them are written not in english!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "tBq0ZLilFF_R"
      },
      "source": [
        "#### Games more played:\n",
        "In our dataset there is a column in which is stored the time played by that player to that particular game. So we have decided to explore what are the games more played in terms of hours. We have decided to pick the top 20 games because we have thought that 20 is a good trade-off between a clear plot and a meaningful number of games. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "IOsK_c-SFF_R"
      },
      "source": [
        "#dataset_8 = dataset_8[[\"author.steamid\", \"author.playtime_forever\",\"app_name\"]]\n",
        "dataset_8 = pd.Series(dataset.groupby(\"app_name\")[\"author.playtime_forever\"].sum().sort_values(ascending=False))\n",
        "ore_di_gioco = dataset_8.values\n",
        "giochi = dataset_8.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "CxF2X3V1FF_S"
      },
      "source": [
        "plt.figure(figsize = ((15, 8)))\n",
        "sns.barplot(x = ore_di_gioco[:20], \n",
        "            y = giochi[:20], orient = 'h')\n",
        "plt.title('TOP 20 games more played in terms of hours', size = 20)\n",
        "plt.ylabel('Games', size = 14, style = 'italic')\n",
        "plt.xlabel('Number of hours', size = 14, style = 'italic')\n",
        "#plt.xscale('log')\n",
        "plt.xticks(np.arange(1000000000,60000000000,2000000000)) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "kmJ7xcQwFF_S"
      },
      "source": [
        "In this barplot we have found some confirms: the games more played are also often the games more reviewed that were appeared in the pie chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "CTj821r-FF_S"
      },
      "source": [
        "#### Active players:\n",
        "To conclude this first analysis we have tried to understand what are the players more useful for Steam: we have selected the ten authors that have written the most number of helpful and funny reviews. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "3Emu42VgFF_S"
      },
      "source": [
        "dataset_9 = pd.Series(dataset[(dataset.votes_helpful > 0)].groupby(\"author.steamid\").votes_helpful.count().sort_values(ascending=False))\n",
        "\n",
        "dataset_10 = pd.Series(dataset[(dataset.votes_funny > 0)].groupby(\"author.steamid\").votes_funny.count().sort_values(ascending=False))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "t_vnvomeFF_T"
      },
      "source": [
        "pd.concat([dataset_9[:11], dataset_10[:11]], axis=1).reset_index().fillna(0).sort_values(by=['votes_helpful'],ascending=False).reset_index(drop = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "zM55NgilFF_T"
      },
      "source": [
        "It's interesting to see that the authors who have written some funny reviews have also written helpful reviews. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "jo1b2tlPFF_T"
      },
      "source": [
        "#### Languages and subplots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "viJ2o2M5FF_T"
      },
      "source": [
        "print(\"The total number of languages used to write reviews is \",'\\033[1m' +str(len(dataset[\"language\"].unique())) +'\\033[0m')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "M6pz0BDfFF_T"
      },
      "source": [
        "Making a subplot we have been able to visualize all the present languages in the dataset and counting the number of reviews. The two subplots have different measure in y-scales!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bzFGCFgJFF_T"
      },
      "source": [
        "fig=plt.figure(figsize=(25,18))\n",
        "ax1=fig.add_subplot(2,1,1)\n",
        "dataset['language'].value_counts().head(10).plot.bar(figsize = (18, 10),title='Top Languages',xlabel='Language',ylabel='Number of Reviews', ax = ax1,rot=0, logy = True, color = \"orange\")\n",
        "ax2=fig.add_subplot(2,1,2)\n",
        "dataset['language'].value_counts().iloc[-18:].plot.bar(figsize = (18, 10),title='Other Languages',xlabel='Language',ylabel='Number of Reviews', ax = ax2,rot=0, color = \"orchid\")\n",
        "fig.tight_layout();\n",
        "\n",
        "#dataset['language'].value_counts().plot.bar(figsize = (18, 7),title='Top Languages',xlabel='Language',ylabel='Number of Reviews', ax = ax1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "RrgR8zBIFF_T"
      },
      "source": [
        "# RQ2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "lD4sQEw2FF_T"
      },
      "source": [
        "### Number of reviews for each application in descending order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gozkpY9bFF_U"
      },
      "source": [
        "We have decided to make a barplot in which we have counted the number of reviews for the first 50 applications. We have decided 50 because it have seemed to us a good tradeoff to have a clean representation a pick the more reviewed games"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "6ubK-7gFFF_U"
      },
      "source": [
        "number_review = dataset.groupby(\"app_name\").review_id.count().sort_values(ascending=False)\n",
        "number_review[0:51].plot.bar(figsize = (18, 7), title=' Number of review', xlabel='Name of application',\n",
        "ylabel='Number of review', color = \"coral\", logy = True)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gIKBOX9rFF_U"
      },
      "source": [
        "### Best Weighted Vote Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "JjlBIbqSFF_U"
      },
      "source": [
        "Each review has a **Weighted Vote Score** that represents the helpfuness score of that review. To extract the weighted  vote score for each game we have computed the mean between all the vote for each application. In this way we have an idea about what applications have received the most helpfulness reviews. Then we have decided to select only average votes above 0.3 because we have considered it a good threshold for the best votes.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "oaynLP7FFF_U"
      },
      "source": [
        "medie = pd.DataFrame(dataset.groupby(\"app_name\").weighted_vote_score.mean().sort_values(ascending=False))\n",
        "medie = medie[medie.values > 0.3]\n",
        "medie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "eRvOQqnQFF_V"
      },
      "source": [
        "# *NEW*\n",
        "### Which applications have the most and the least recommendations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "oR4tUipUFF_W"
      },
      "source": [
        "#Most\n",
        "# recommended. group_by app_name. count all recommended,\n",
        "# count True recommended and False recommended in separate cols, and percentage of these.\n",
        "# taking only the useful cols\n",
        "new_data = dataset[['app_name', 'recommended']]\n",
        "# count_rec col counts all recommended respectively False and True of an application\n",
        "new_data['count_rec'] = new_data.groupby(['app_name', 'recommended'], sort=False)['recommended'].transform('count')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "kMA_ooNmFF_W"
      },
      "source": [
        "# first 20\n",
        "new_data.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "CufO2VZGFF_W"
      },
      "source": [
        "# all_rec col counts all recommedations, False and True together\n",
        "new_data['all_rec'] = new_data.groupby(\"app_name\", sort=False)['count_rec'].transform('count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "B2gLTFRiFF_W"
      },
      "source": [
        "# first 20\n",
        "new_data.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "-d-3F1lGFF_W"
      },
      "source": [
        "# final dataframe which contains only the True recommendations\n",
        "# this means that we can calculate the most and the least recommended apps\n",
        "final = new_data[(new_data['recommended']==True)].drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "_nFmeTznFF_W"
      },
      "source": [
        "# first 20\n",
        "final.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "0hloC-GfFF_X"
      },
      "source": [
        "# perc_rec calculates the percentage recommendation\n",
        "final['perc_rec'] = (final['count_rec']/final['all_rec'])*100\n",
        "# drop not useful cols\n",
        "final.drop(['recommended', 'count_rec'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "4cs7DXgTFF_X"
      },
      "source": [
        "# most recommended, first 50\n",
        "final.sort_values(by='perc_rec', ascending=False).head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "_51QZCvqFF_X"
      },
      "source": [
        "# least recommended, first 50\n",
        "final.sort_values(by='perc_rec', ascending=True).head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "-viFjRm7FF_X"
      },
      "source": [
        "### How many of these applications were purchased, and how many were given for free?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "l18pkt0FFF_X"
      },
      "source": [
        "# steam_purchase\n",
        "# taking only the useful cols\n",
        "new_data1 = dataset[['app_name', 'steam_purchase']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Xre7wwmHFF_Y"
      },
      "source": [
        "# first 20\n",
        "new_data1.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "0I-QtMXBFF_Y"
      },
      "source": [
        "# same modus operandi of counting recommendation\n",
        "new_data1['count_pur'] = new_data1.groupby(['app_name', 'steam_purchase'], sort=False)['steam_purchase'].transform('count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "P-ngkxlPFF_Y"
      },
      "source": [
        "# first 20\n",
        "new_data1.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "SdlPjGiZFF_Y"
      },
      "source": [
        "# taking only the ones purchased\n",
        "final1 = new_data1[(new_data1['steam_purchase']==True)].drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ZkoqM0tZFF_Z"
      },
      "source": [
        "# first 20\n",
        "final1.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "lyvVo1t2FF_Z"
      },
      "source": [
        "# drop not useful col\n",
        "final1.drop(['steam_purchase'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "GHUok0WhFF_Z"
      },
      "source": [
        "# first 20\n",
        "final1.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9grdGASeFF_Z"
      },
      "source": [
        "# received_for_free\n",
        "# taking only the useful cols\n",
        "new_data2 = dataset[['app_name', 'received_for_free']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "wuChogPTFF_Z"
      },
      "source": [
        "# first 20\n",
        "new_data2.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "QM26eklXFF_Z"
      },
      "source": [
        "# same modus operandi\n",
        "new_data2['count_free'] = new_data2.groupby(['app_name', 'received_for_free'], sort=False)['received_for_free'].transform('count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "hR_hymUcFF_Z"
      },
      "source": [
        "# first 20\n",
        "new_data2.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "C6DIAWE9FF_Z"
      },
      "source": [
        "# take only the ones received_for_free\n",
        "final2 = new_data2[(new_data2['received_for_free']==True)].drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "YKOqjHhXFF_a"
      },
      "source": [
        "# first 20\n",
        "final2.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "HK1QOhk_FF_a"
      },
      "source": [
        "# drop not useful col\n",
        "final2.drop(['received_for_free'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "iwtWG3P-FF_a"
      },
      "source": [
        "# first 20\n",
        "final2.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "-wVJHu_OFF_a"
      },
      "source": [
        "# now it's time to calculate the final result, by doing a merge of the final dataframes\n",
        "dfs = [final, final1, final2]\n",
        "final_df = reduce(lambda  left,right: pd.merge(left,right,on=['app_name'],\n",
        "                                            how='outer'), dfs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "eApm7W0XFF_a"
      },
      "source": [
        "# sorting the values in descending order\n",
        "final_df.sort_values(by='perc_rec', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "smqeTAvQFF_a"
      },
      "source": [
        "# taking the first 40 apps that are most recommended and displaying how many times were\n",
        "# purchased and how many times were received for free\n",
        "final_df.head(40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "l3FVgXJhFF_b"
      },
      "source": [
        "# *OLD*\n",
        "### Which applications have the most and the least recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "u-TT8HBWFF_b"
      },
      "source": [
        "Just to start we have just selected the ten applications that have more and less number of positive recommendations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "aBoEhPewFF_b"
      },
      "source": [
        "#Most\n",
        "most = pd.DataFrame(dataset[(dataset.recommended == True)].groupby(\"app_name\").recommended.count().sort_values(ascending=False)).rename(columns={'recommended': 'positive rec'})\n",
        "most.iloc[:10]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "MMvbV6maFF_b"
      },
      "source": [
        "#Least\n",
        "most.iloc[-10:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "JdrnA0KMFF_b"
      },
      "source": [
        "After extracting the absolute values we have decided to compute the relative values of number of positive recommendations. We have done that computing for each application the ratio between positive recommendation and total number of reviews. This kind of operation have seemed more meaningful to us"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1OkIYUJLFF_c"
      },
      "source": [
        "total_recom = dataset.groupby('app_name')['recommended'].count().reset_index()\n",
        "\n",
        "positive_recom = dataset[(dataset.recommended == True)].groupby('app_name')['recommended'].count().reset_index()\n",
        "\n",
        "positive_recom = positive_recom.rename(columns={'app_name': 'app_name', 'recommended': 'rec_True'})\n",
        "\n",
        "dataset_12= positive_recom.merge(total_recom, left_on = 'app_name', right_on = 'app_name')\n",
        "\n",
        "dataset_12['percent_True'] = dataset_12['rec_True'] / dataset_12['recommended']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "wNAYaEy3FF_c"
      },
      "source": [
        "dataset_12 = dataset_12.sort_values(by=[\"percent_True\"], ascending = False).reset_index(drop = True)\n",
        "\n",
        "most_2 = dataset_12.iloc[0:10]\n",
        "\n",
        "least_2 = dataset_12.iloc[-10:]\n",
        "\n",
        "most_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "RcNkxUyeFF_c"
      },
      "source": [
        "Just to visualize this table we have made a plot in which we can see the ratio between positive recommendation and total recommendations of games and in addition we can have also an idea of their absolute value. Not all games have the same number of recommendations!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "iFaQh--wFF_c"
      },
      "source": [
        "ax = most_2.plot(x = \"app_name\", y = [\"rec_True\", \"recommended\"], kind ='bar', stacked = False, figsize =(10,8), alpha=1, rot=90, color =[\"bisque\",\"limegreen\"])\n",
        "ax.set_yscale('log')\n",
        "ax.set_xlabel('languages')\n",
        "ax.set_ylabel(\"number reviews\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "UnHHTGRpFF_c"
      },
      "source": [
        "least_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "LwTkV4AVFF_c"
      },
      "source": [
        "We do the same kind of plot for games that have the smaller ratio between positive recommendation and total recommendation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1j-UrBVnFF_c"
      },
      "source": [
        "ax = least_2.plot(x = \"app_name\", y = [\"rec_True\", \"recommended\"], kind ='bar', stacked = False, figsize =(10,8), alpha=1, rot=90, color = [\"lightblue\",\"purple\"])\n",
        "ax.set_yscale('log')\n",
        "ax.set_xlabel('languages')\n",
        "ax.set_ylabel(\"number reviews\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "JKZu1gICFF_d"
      },
      "source": [
        "We can see that if we consider the absolute or relative value of positive recommendations we extract different games. When we consider the absolute value there are some games that have an high number of positive recommendations but also not positive ones so they are in the both list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "FZ2MMauJFF_d"
      },
      "source": [
        "### How many of these applications were purchased, and how many were given for free?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "FYL2Htk8FF_d"
      },
      "source": [
        "Among applications that have the major and the minor *relative* number of positive recommendations we have observed for each application the number of copies that are received for free and the one which are not. We have used only the column **received_for_free** because we have thought that if in this column there is a **True** the game is received for free if instead there is **False** the game is purchased.\n",
        "\n",
        "We have considered games that have more positive recommendations relative to their number of reviews and not considered games that have more positive recommendations in absolute value because we have thought that in this way the dataframe that we will obtain are more meaningful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "NYHVlv5BFF_d"
      },
      "source": [
        "most_1 = list(most_2.app_name)\n",
        "new_data = dataset[(dataset[\"app_name\"].isin(most_1))]\n",
        "pd.DataFrame(new_data.groupby([\"app_name\", \"received_for_free\"]).received_for_free.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "yYza6oumFF_d"
      },
      "source": [
        "least_1 = list(least_2.app_name)\n",
        "new_data_2 = dataset[(dataset[\"app_name\"].isin(least_1))]\n",
        "pd.DataFrame(new_data_2.groupby([\"app_name\", \"received_for_free\"]).received_for_free.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "GiX4a4yDFF_d"
      },
      "source": [
        "#new_data[new_data.received_for_free == True].groupby(\"app_name\").received_for_free.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KhWrqI_1FF_e"
      },
      "source": [
        "#Most\n",
        "#most_1 = list(most.index)\n",
        "#new_data = new_data[(new_data[\"app_name\"].isin(most_1))]\n",
        "#pd.DataFrame(new_data.groupby([\"app_name\", \"received_for_free\"]).recommended.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "5JHhlkQyFF_e"
      },
      "source": [
        "#Least\n",
        "#least_1 = list(least.index)\n",
        "#new_data1 = new_data1[(new_data1[\"app_name\"].isin(least_1))]\n",
        "#pd.DataFrame(new_data1.groupby([\"app_name\", \"received_for_free\"]).recommended.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ywoNDphfFF_f"
      },
      "source": [
        "# RQ 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ziQv8KOcFF_f"
      },
      "source": [
        "### What is the most common time that authors review an application? For example, authors usually write a review at 17:44."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "wQm0h0FoFF_g"
      },
      "source": [
        "# first point\n",
        "# taking only the timestamp_created col\n",
        "timestamp_col = np.array(dataset[\"timestamp_created\"].dt.time.astype('str'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "vhHSxDbrFF_g"
      },
      "source": [
        "dict_time = {}\n",
        "for time in timestamp_col:\n",
        "    # taking only hour and minute\n",
        "    new_time = time[:5]\n",
        "    if new_time not in list(dict_time.key()):\n",
        "        dict_time[new_time] = 1\n",
        "    else:\n",
        "        dict_time[new_time] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7pr-zSoHFF_g"
      },
      "source": [
        "# sorting the dictionary in descending order\n",
        "dict_time_sorted = {k: v for k, v in sorted(dict_time.items(), key=lambda item: item[1], reverse=True)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "uPnA8FcRFF_g"
      },
      "source": [
        "# returning the most common time (without seconds)\n",
        "next(iter(dict_time_sorted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "2C1SLX3qFF_g"
      },
      "source": [
        "### Create a function that receives as a parameter a list of time intervals and returns the plot the number of reviews for each of the intervals.\n",
        "\n",
        "Using the function **orario** we can extract for a given list of time interval the number of reviews written in each time interval \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "v0pg9gkxFF_g"
      },
      "source": [
        "### Use the function that you created in the previous literal to plot the number of reviews between the following time intervals:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "x4tdouiIFF_g"
      },
      "source": [
        "intervalli = ['06:00:00', '10:59:59', '11:00:00', '13:59:59', '14:00:00', '16:59:59',\n",
        "        '17:00:00', '19:59:59', '20:00:00', '23:59:59', '00:00:00', '02:59:59', '03:00:00',\n",
        "        '05:59:59']\n",
        "timestamp_col1 = np.array(dataset[\"timestamp_created\"].dt.time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bR852bw0FF_h"
      },
      "source": [
        "orario(intervalli, timestamp_col1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "FIXpMFGjFF_h"
      },
      "source": [
        "On the x-axis for each bar is indicated the starting point of the time interval. We have observed that fewer people have written reviews during the night while the majority of people have written their reviews in the first hours of the morning and in the dinner hours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "qb2TxxNlFF_h"
      },
      "source": [
        "# RQ4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "r4inZ3tXFF_h"
      },
      "source": [
        "### What are the top 3 languages used to review applications?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "yoV6VJRRFF_h"
      },
      "source": [
        "top_languages = pd.DataFrame(dataset.groupby(\"language\").review_id.count().sort_values(ascending=False).head(3))\n",
        "top_languages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "itBQIucvFF_h"
      },
      "source": [
        "As expected the majority of the reviews are written in english, chinese and russian!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "RoQiviacFF_h"
      },
      "source": [
        "top_languages = list(top_languages.index)\n",
        "top_languages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "cCVXOhsAFF_h"
      },
      "source": [
        "### Create a function that receives as parameters both the name of a data set and a list of languagesâ€™ names and returns a data frame filtered only with the reviews written in the provided languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "7KgZVZiqFF_i"
      },
      "source": [
        "There we have used the function **filtro** to accomplish a dataframe where there are only reviews written in the top 3 languages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "skfqIrRSFF_i"
      },
      "source": [
        "dataset_filter = filtro(dataset, top_languages)\n",
        "dataset_filter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "1jeQWX2bFF_j"
      },
      "source": [
        "### Use the function created in the previous literal to find what percentage of these reviews (associated with the top 3 languages) were voted as funny?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "y6IfHT67FF_j"
      },
      "source": [
        "For this request we have used the new filtered dataset and for each language we have selected the reviews that have received at least one funny vote and then we have computed the ratio between them and all the reviews written in that language.\n",
        "\n",
        "To compute this percentage we have used **dataset_filter** that is the new dataframe obtained using the previous function **filtro**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "JqQ2ZK0XFF_j"
      },
      "source": [
        "numeratore = []\n",
        "denominatore = []\n",
        "rapporto = []\n",
        "for i in range(len(top_languages)):\n",
        "    numeratore.append(dataset_filter[(dataset_filter.votes_funny != 0) & (dataset_filter.language == top_languages[i])].votes_funny.count())\n",
        "    denominatore.append(dataset_filter[dataset_filter.language == top_languages[i]].votes_funny.count())\n",
        "    rapporto.append(round((numeratore[i]/denominatore[i])*100, 2))\n",
        "    print(\"The percentage of reviews written in \",'\\033[1m' +top_languages[i]+'\\033[0m',\" that has received at least a funny vote is \"'\\033[1m' +str(rapporto[i])+\"%\"+'\\033[0m')\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "CQOaxXkOFF_j"
      },
      "source": [
        "At this point we have also wanted to compute the percentage of reviews that have received at least a funny vote among all these three languages. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "wqwbUv4CFF_j"
      },
      "source": [
        "num = dataset_filter[dataset_filter.votes_funny != 0].votes_funny.count()\n",
        "den = dataset_filter.votes_funny.count()\n",
        "print(\"The percentage of reviews written in one of the top 3 language that has received at least a funny vote is \", '\\033[1m' +str(round((num/den)*100, 2))+\"%\"+'\\033[0m')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "NUUpdBfRFF_j"
      },
      "source": [
        "### Use the function created in the literal â€œaâ€ to find what percentage of these reviews (associated with the top 3 languages) were voted as helpful?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "tByN63baFF_k"
      },
      "source": [
        "For this request we have used the new filtered dataset and for each language we have selected the reviews that have received at least one helpful vote and then we have computed the ratio between them and all the reviews written in that language.\n",
        "\n",
        "To compute this percentage we have used **dataset_filter** that is the new dataframe obtained using the previous function **filtro**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1fuJFSA6FF_k"
      },
      "source": [
        "numeratore = []\n",
        "denominatore = []\n",
        "rapporto = []\n",
        "for i in range(len(top_languages)):\n",
        "    numeratore.append(dataset_filter[(dataset_filter.votes_helpful != 0) & (dataset_filter.language == top_languages[i])].votes_helpful.count())\n",
        "    denominatore.append(dataset_filter[dataset_filter.language == top_languages[i]].votes_helpful.count())\n",
        "    rapporto.append(round((numeratore[i]/denominatore[i])*100, 2))\n",
        "    print(\"The percentage of reviews written in \",'\\033[1m' +top_languages[i]+\"%\"+'\\033[0m'+ \" that has received at least a helpful vote is \",'\\033[1m' +str(rapporto[i])+\"%\"+'\\033[0m'+\"%\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "xhio9kCMFF_k"
      },
      "source": [
        "At this point we have also wanted to compute the percentage of reviews that have received at least a helpful vote among all these three languages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "UoCdyToRFF_k"
      },
      "source": [
        "num = dataset_filter[dataset_filter.votes_helpful != 0].votes_helpful.count()\n",
        "den = dataset_filter.votes_helpful.count()\n",
        "print(\"The percentage of reviews written in one of the top 3 language that has received at least a helpful vote is \", '\\033[1m' +str(round((num/den)*100, 2))+\"%\"+'\\033[0m')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "N5SXhlObFF_k"
      },
      "source": [
        "# RQ6 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "euOm1HhvFF_k"
      },
      "source": [
        "### What is the average time (days and minutes) a user lets pass before he updates a review?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "vhiIi-c7FF_l"
      },
      "source": [
        "Just to start we have computed the difference between the time when the review is written and time when the review is updated and then we have transformed this difference in terms of days"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "wGbJsT7pFF_l"
      },
      "source": [
        "dataset['Difference_Days'] = (dataset['timestamp_updated'] - dataset['timestamp_created'])\n",
        "dataset['Difference_Days'] = dataset['Difference_Days']/np.timedelta64(1,'D')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "XZXcyUwVFF_l"
      },
      "source": [
        "After that we have deleted who did not update his review because we have thought that is meaningless consider them. Then we have computed the mean between days and the integer part of this number represents the average number of days after an author updates his review. Instead to transform the decimal part in minutes we have to multiply it for 1440 because in one day there are 1440 minutes. We have made a simple proportion: *1 : 1440 = x : (decimal part of our number)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "vwZJ19ALFF_l"
      },
      "source": [
        "dataset_1 = dataset[dataset.Difference_Days != 0]\n",
        "average = dataset_1.Difference_Days.mean()\n",
        "minutes = round((average % 1) * 1440, 0)\n",
        "days = average // 1\n",
        "print(\"The average time a user lets pass before he updates a review is \",'\\033[1m' +str(days)+'\\033[0m' +\" days and \",'\\033[1m' +str(minutes)+'\\033[0m' +\" minutes\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "unyXYiHTFF_m"
      },
      "source": [
        "On average an author updates his review almost after a year! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "N-H8wQntFF_m"
      },
      "source": [
        "### Plot the top 3 authors that usually update their reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ug8_fnmmFF_m"
      },
      "source": [
        "We have used the dataframe **dataset_1** in which there are only the reviews that have been updated. We did not use the starting dataset because we have to extract who are the authors that usually update their reviews so authors that have updated more reviews through time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "PFn6cQELFF_m"
      },
      "source": [
        "a = pd.Series(dataset_1.groupby('author.steamid').review_id.count().sort_values(ascending=False).head(3))\n",
        "a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "lxC6-ZWcFF_m"
      },
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "ax = a.plot(kind=\"bar\", color = [\"orchid\", \"orange\", \"green\"], alpha=0.75, rot=0)\n",
        "ax.set_title(\"TOP 3 authors that have updated more reviews\")\n",
        "ax.set_xlabel(\"Steam ID\")\n",
        "ax.set_ylabel(\"Number of reviews updated\")\n",
        "labels = list(a.values)\n",
        "for rect, label in zip(rects, labels):\n",
        "    height = rect.get_height()\n",
        "    ax.text(\n",
        "        rect.get_x() + rect.get_width() / 2, height + 1, label, ha=\"center\", va=\"bottom\"\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "fTbGocJtFF_m"
      },
      "source": [
        "We have put the number of reviews over the bars because the second and the third author have updated almost the same number of reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "kspC0zScFF_m"
      },
      "source": [
        "# RQ7\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gb5NRzYuFF_m"
      },
      "source": [
        "### Whatâ€™s the probability that a review has a Weighted Vote Score equal to or bigger than 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "QhesegpyFF_n"
      },
      "source": [
        "We have used the definition of probability to compute these values indeed we have count the number of reviews that has a Weighted Vote Score equal to or bigger than 0.5 and this number represents the favourable case (we have stored this number in **casi_fav**)while the number of total case is represented by the number of the lines of our dataset, stored in **casi_tot**. The probability is the ratio between them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "4fadFcqJFF_n"
      },
      "source": [
        "casi_fav = dataset[dataset.weighted_vote_score >= 0.5].weighted_vote_score.count()\n",
        "casi_fav"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "TC0MUg1lFF_n"
      },
      "source": [
        "casi_tot = dataset.weighted_vote_score.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "00gKCeJ_FF_n"
      },
      "source": [
        "result_1 = round(casi_fav/casi_tot, 2)\n",
        "print(\"The probability is of a review has a Weighted Vote Score equal to or bigger than 0.5 is \"+ '\\033[1m' +str(result_1)+'\\033[0m')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "RDVvRFFYFF_n"
      },
      "source": [
        "### Whatâ€™s the probability that a review has at least one vote as funny given that the Weighted Vote Score is bigger than 0.5?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "EcW_yT9NFF_n"
      },
      "source": [
        "To compute this conditional probability my sample sample will be reduced indeed we have filtered the dataset in such way that we are going to look for reviews with at least one vote as funny just among reviews with Weighted Vote Score is bigger than 0.5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "OR7vliRNFF_n"
      },
      "source": [
        "dataset_prob = dataset[dataset.weighted_vote_score > 0.5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "er1fqwjBFF_n"
      },
      "source": [
        "casi_fav_2 = dataset_prob[dataset_prob.votes_funny != 0].votes_funny.count()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "_8lQirpyFF_o"
      },
      "source": [
        "Now our sample space in other words the total case are the favourable case used to compute the last probability, **case_fav**: number of reviews that has a Weighted Vote Score equal to or bigger than 0.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bD1upiNdFF_o"
      },
      "source": [
        "result_2 = round(casi_fav_2/casi_fav, 2)\n",
        "print(\"The conditional probability that a review has at least one vote as funny given that the Weighted Vote Score is bigger than 0.5 is \",'\\033[1m' +str(result_2)+'\\033[0m')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "o9YwtAJ0FF_o"
      },
      "source": [
        "### Is the probability that â€œa review has at least one vote as funnyâ€ independent of the â€œprobability that a review has a Weighted Vote Score equal or bigger than 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "jia-FU9yFF_o"
      },
      "source": [
        "To be independent these two events it would happen that the probability of the event: *a review has at least one vote as funny* would be equal to *probability that a review has at least one vote as funny given that the Weighted Vote Score is bigger than 0.5* because in this way the conditioning of the two probability is useless given that they are independent. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "uZSgFf-8FF_o"
      },
      "source": [
        "casi_fav_3 = dataset[dataset.votes_funny != 0].votes_funny.count()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Sk8zogBbFF_o"
      },
      "source": [
        "result_3 = round(casi_fav_3/casi_tot,2)\n",
        "print(\"The probability of a review has at least one vote as funny is \"+ '\\033[1m' +str(result_3)+'\\033[0m')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "8ivo6s5dFF_o"
      },
      "source": [
        "0.12 is different from 0.24 so these two events are **dependent!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-3XAbcC4NzQ"
      },
      "source": [
        "#$TQ_1$\n",
        "##Question 1\n",
        "As known, given a random variable $X$, the Quantile function *Q($\\cdot$)* with support $\\{ p | p \\in [0,1] \\}$ is the function that computes:\n",
        "$$\n",
        "    Q(p)=s \\hspace{0.2 cm} |\\hspace{0.2 cm} \\mathcal{P}(X<=s) = p\n",
        "$$\n",
        "Denoting with $A_i$ the i-th element of the vector $A$ of length $n$ and given $k \\in [0,n]$, it is possible to see that our algorithm compute:\n",
        "$$\n",
        "    alg(A,k)=s \\hspace{0.2 cm} |\\hspace{0.2 cm} \\#\\{A_i<=s\\} = k\n",
        "$$\n",
        "It is then easily possible to perform some trasformations over our algorithm parameters in order to obtain the similarities with the quantile function, i.e.:\n",
        "\n",
        "1. A shrinkage over our algorithm support space (i.e. $k'=k/n$);\n",
        "    \n",
        "2. A shrinkage over our cardinality measure (i.e. $\\#\\{A_i<=s \\}'=\\frac{\\#\\{A_i<=s \\}}{n}$);\n",
        "\n",
        "Substituting into our $alg(A,k)$ it becomes:\n",
        "\\begin{equation}\n",
        "    alg(A,k')=s\\hspace{0.2 cm} |\\hspace{0.2 cm} \\frac{\\#\\{A_i<=s\\}}{n} = k'\n",
        "\\end{equation}\n",
        "In a frequentist approach (said $A_r$ a random sample of the vector $A$) we can equal $\\frac{\\#\\{A_i<=s\\}}{n}= \\mathcal{P}(A_r <= s)$; In words, our algorithm is computing the value $s$ so that the number of elements in the array $A$ smaller or equal to $s$ will be equal to $k$: we can so somehow define our algorithm a \"quantile function over a non-normalized support\".\n",
        "##Question 2\n",
        "Let consider the worst case scenario, i.e. imagine that $k=n$ and that at each iteration the random sample $s$ will always be equal to $A_1$: it basically means that the $s$ satisfying the condition over $k$ will be selected at the $n_{th}-1$ iteration (when the vector $A$ over which we are calling $alg()$ has lenght equal to 2), we can then assume an asymptotical complexity in the worst case scenario (removing costant therms) equal to $\\mathcal{O}(n)$.\n",
        "##Question 3\n",
        "In the best case scenario, the right $s$ will always be picked up at the first iteration, regardless of $n$=len($A$): the asymptotical complexity will then be equal to $\\mathcal{O}(1)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysb4ODti98J0"
      },
      "source": [
        "#$TQ_2$\n",
        "##Question 1\n",
        "Let dive into the interpretation of the given recursive algorithm's complexity. It is clear that, given a particular $n$ and $\\forall l$, and expressing with $T(n)$ the time needed to complete the algorithm called with parameter $n$:\n",
        "\n",
        "\\begin{equation}\n",
        "    T(n) = T\\left(\\frac{n}{2}\\right)\\cdot 2 + \\left(\\frac{n}{2}+1\\right)\\cdot 3\n",
        "\\end{equation}\n",
        "\n",
        "Infact, calling **splitSwap(a,l,n)** we will have to solve two times **splitSwap(a,l,n/2)** plus execute 3 operations for each of the $\\left(\\frac{n}{2}+1\\right)$ iterations of the for loop into **swapList(a,l,n)**. Lets compute running times after the expression of $T(n)$:\n",
        "\n",
        "\\begin{equation}\n",
        "    T\\left(\\frac{n}{2}\\right) = T\\left(\\frac{n}{2^2}\\right)\\cdot 2 + \\left(\\frac{n}{2^2}+1\\right)\\cdot 3\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "    T(n) = T\\left(\\frac{n}{2^2}\\right)\\cdot 2^2 + \\left(\\frac{n}{2^2}+1\\right)\\cdot2 \\cdot 3 +\\left(\\frac{n}{2}+1\\right)\\cdot 3\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "    T(n) = T\\left(\\frac{n}{2^2}\\right)\\cdot 2^2 + \\left(\\frac{n}{2}+1\\right)\\cdot2 \\cdot 3 +3\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "    T\\left(\\frac{n}{2^2}\\right) = T\\left(\\frac{n}{2^3}\\right)\\cdot 2 + \\left(\\frac{n}{2^3}+1\\right)\\cdot 3\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "    T(n) = T\\left(\\frac{n}{2^3}\\right)\\cdot 2^3 + \\left(\\frac{n}{2}+1\\right)\\cdot 3 \\cdot 3 +7\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "    T(n) = T\\left(\\frac{n}{2^k}\\right)\\cdot 2^k + \\left(\\frac{n}{2}+1\\right)\\cdot k \\cdot 3 +log_2(2^k)-1\n",
        "\\end{equation}\n",
        "\n",
        "Setting $2^k=n \\Leftrightarrow k =log_2(n)$ we obtain:\n",
        "\n",
        "\\begin{equation}\n",
        "    T(n) = T(1)\\cdot n + \\left(\\frac{n}{2}+1\\right)\\cdot log_2(n) \\cdot 3 +log_2(n)-1 \\simeq n\\cdot log_2(n)\n",
        "\\end{equation}\n",
        "\n",
        "In the latter we have removed the dependency from factors, constant terms and considered only the term with the biggest growth rate w.r.t $n$. We can than say that the asymptotical complexity of the algorithm is $\\mathcal{O}(n\\cdot log_2(n))$.\n",
        "\n",
        "#Question 2\n",
        "Given an array **a**, an index **l** and a number **n** (considering the scenario where both **len(a)** and **n** are power of 2 numbers), the algorithm output the array **a'** built as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "    a'[i]=a[i] \\hspace{1cm}\\forall i \\in [0,1,...,l-1]\\hspace{1cm}\\mbox{if}\\hspace{1cm} l \\geq 1\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "     a'[l+i]=a[l+n-i]\n",
        "\\end{equation}\n",
        "\n",
        "In words, starting from an index **l** of the original array **a**, the algorithm is reversing the position of the first **n** elements of the array. Because of this of course it is required that **l+n** $\\leq$ **len(a)**, otherwise the subroutine **swapList()** will raise an error because of the out-of-range index it loops on. Let describe the algorithm's mechanism. Looking at the code, we can assess how the only part of the code actually changing the position of the array's elements is the subroutine **swapList()**. Given a triplet **(a,l,n)**, once **splitSwap()** is called, it will recursively call himself with an **n** halfed call by call (i.e. **n**$^{(1)}$ =**n/2**, **n**$^{(2)}$ =**n**$^{(1)}/2$, **n**$^{(3)}$ =**n**$^{(2)}/2$ and so on). As we can see in the (Fig.1), after $\\text{log}_2(n)-1$ steps, the function **splitSwap(a,l,2)** will be called: in its execution both **splitSwap(a,l,1)** and **splitSwap(a,l+1,1)** will **return** (being **n**=1), finally allowing the execution of **swaplist(a,l,2)** (that we will call **final-node-subroutine** $\\forall l$) that will exchange the position of the array's elements **a[l]** with **a[l+1]**. Being  **splitSwap(a,l,2)** completed,  **splitSwap(a,l+2,2)** will be called. Similary, at the end of the execution its **final-node-subroutine** will exchange the position of the array's elements **a[l+2]** with **a[l+3]**. Basically the **final-node-subroutines** consider the array (starting from the element $a[l]$) as a sequence of $\\frac{n}{2}$ couples of elements and in each couple they exchange the 1st element with the 2nd one.\n",
        "\n",
        "Recalling that **splitSwap(a,l,2)** and **splitSwap(a,l+2,2)** where called in **splitSwap(a,l,4)**, **swapList(a,l,4)** (that we will call **semi-final-node-subroutine**) will finally be executed, exchanging the position of the array's elements **a[l]** with **a[l+2]** and **a[l+1]** with **a[l+3]**. So the role of **semi-final-node-subroutines** is to consider the array (starting from the element $a[l]$) as a sequence of $\\frac{n}{4}$ couples of couples and to exchange the position of the 1st element of the 1st couple with the 1st element of the 2nd couple, and the 2nd element of the 1st couple with the 2nd element of the 2nd couple. Basically, after the execution of all the **final-node-subroutines** and of the  **semi-final-node-subroutines** the position of the 1st group of 4 elements of the original array will be reversed, the same for the 2nd group of 4 elements and so on. We can so climb our recursive function tree from the **final-node-subroutines** up to the top **first-final-node-subroutine** i.e. **swapList(a,l,n)**. We can see the effect of each kind of **subroutine** level over a test array in two examples at (Fig.2,3) recalling that the output of the **first-final-node-subroutine** will be equal to the algorithm's output.\n",
        "\n",
        "Having assessed that the algorithm complexity is $\\simeq O(n\\cdot log_2(n))$, it is possible to confirm that the algorithm it's not optimal: infact it is easily possible to write some pseudo-code with a lower complexity than the given algorithm:\n",
        "\n",
        "$\\hspace{1cm}$**def reverse(a,l,n):**\n",
        "\n",
        "$\\hspace{2cm}$**reversed_array=a**\n",
        "\n",
        "$\\hspace{2cm}$**for i in range(n):**\n",
        "\n",
        "$\\hspace{3cm}$**reversed_array[i+l]=a[l+n-i]**\n",
        "\n",
        "$\\hspace{2cm}$**return reversed_array**\n",
        "\n",
        "We can easily see that the **reverse()** algorithm complexity has now become (removing costant therms and factors) $O(n)$, proving that the **splitSwap()** algorithm was not optimal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY6wcZHfAAiq"
      },
      "source": [
        "<figure>\n",
        "\n",
        "  <img src =\"https://drive.google.com/uc?export=view&id=1VP65dIPAoMyp2YGBC59AwacUQynJ7rK_\" width=40%>\n",
        "<figcaption align=\"center\"> Fig.1 :Reaching the first final-node-subroutine</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCGEv8C3Bjf9"
      },
      "source": [
        "<figure>\n",
        "\n",
        "  <img src =\"https://drive.google.com/uc?export=view&id=1a5aDf8nmvmhTVc3KpRu3A072Xc2DWfY6\" width=140%>\n",
        "<figcaption align=\"center\"> Fig.2 :Test over a with len(a)=n=16, l=0</figcaption>\n",
        "</figure>\n",
        "\n",
        "<figure>\n",
        "\n",
        "  <img src =\"https://drive.google.com/uc?export=view&id=1SH6fTN4Xp8Oiowht0D7VSN5ypnbB_oN6\" width=140%>\n",
        "<figcaption align=\"center\"> Fig.3 :Test over a with len(a)=16, n=8, l=7</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GKL5_r-CPx4"
      },
      "source": [
        "#$TQ_3$: Knapsack\n",
        "In this theoretical question we have to face with a NP-complete problem: the Knapsack one. To solve it generally we have to use heuristic solutions but in some cases they fail to provide the optimal solution. \n",
        "* The first heuristic solution is a greedy algorithm in which we order the object in increasing order of weight and then visit them sequentially, adding them to the solution as long as the budget is not exceeded. This algorithm does not provide the optimal solution in every situation indeed in my counterexample this greedy algorithm fails: we fix the budget: **W** = 10 and we have three object.\n",
        "\n",
        "\n",
        "|i    |w_i| v_i|\n",
        "|-----|---|----|\n",
        "|1    |4  |3   |\n",
        "|2    |6  |5   |\n",
        "|3    |10 |9   |\n",
        "\n",
        "We have to visit the object sequentially so we are going to pick the first two objects, but we cannot pick the third one because we will exceed the budget. This choice is not optimal because it would be better pick only the third object because its values (9) is greater of the sum of the first two (8).\n",
        "\n",
        "* In the second heuristic solution we have to order the objects in decreasing order of values, and then visit them sequentially, adding them to the solution if the budget is not exceeded. This algorithm does not provide the optimal solution in each situation indeed in my counterexample this greedy algorithm fails: I have decided to choose the same budget **W** = 10 and the same number of object of the last counterexample. \n",
        "\n",
        "|i    |w_i| v_i|\n",
        "|-----|---|----|\n",
        "|1    |9  |9   |\n",
        "|2    |7  |7   |\n",
        "|3    |3 |3   |\n",
        "\n",
        "We have to visit the objects sequentially so we are going to pick the first object, but we cannot pick the last two because we will exceed the budget. This choice is not optimal because it would be better pick the second and the third objects because the sum of their values (10) is greater of the first object value (9).\n",
        "\n",
        "* In the third heuristic solution we have to order them in decreasing relative value ($v_1$/ $w_i$), and then visit them sequentially, adding them to the solution if the budget is not exceeded\n",
        "This algorithm does not provide the optimal solution in each situation indeed in my counterexample this greedy algorithm fails: I have decided to choose the same budget **W** = 10 and the same number of object of the two last counterexamples. \n",
        "\n",
        "|i    |w_i| v_i|\n",
        "|-----|---|----|\n",
        "|1    |9  |10   |\n",
        "|2    |7  |7   |\n",
        "|3    |3 |3   |\n",
        "\n",
        "We have to visit the objects sequentially so we are going to pick the first object whose relative value is 1.11 while the one of the other objects is 1. We cannot pick the last two because we will exceed the budget. This choice is not optimal because it would be better pick the second and the third objects because the sum of their values (10) is greater of the first object value (9)."
      ]
    }
  ]
}